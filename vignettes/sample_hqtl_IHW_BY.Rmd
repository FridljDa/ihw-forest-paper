---
title: "hQTL analysis with IHW-Benjamini-Yekutieli"
author: "Danie Fridljand"
date: "`r doc_date()`"
output: BiocStyle::html_document
vignette: >
  %\VignetteIndexEntry{"IHW-Forest-paper: hQTL analysis with IHW-Benjamini-Yekutieli"}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r warning=FALSE, message=FALSE}
library(IHW)
library(fdrtool)
library(ggplot2)
library(cowplot)
theme_set(theme_cowplot())
library(tidyr)
library(scales)
library(latex2exp)
library(dplyr)
require(biomaRt)
set.seed(1)
```

```{r setup, include=FALSE, cache = FALSE}
knitr::opts_knit$set(root_dir = "/g/huber/users/fridljand/R/ihw-forest-paper/")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r hard_coding_path}
# file_loc <- system.file("extdata", "real_data", "hqtl_chrom1_chrom2", package = "IHWpaper")
# file_loc <- "/g/huber/users/fridljand/R/ihw-forest-paper/data/hqtl_chrom1_chrom2/"
root_dir <- "/g/huber/users/fridljand/R/ihw-forest-paper/"
# root_dir <- "/Users/default/Google Drive/currentDocumants/Studium/Master/3.Semester/Masterarbeit/Code/ihw-forest-paper/data/hqtl_chrom1_chrom2/"
# root_dir <- "/Volumes/huber/users/fridljand/R/ihw-forest-paper"
options(bitmapType = "cairo")
```

# Load data
In this markdown, we apply several multiple testing procedures to the same hQTL analysis as in https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12411?af=R, Section 6: APPLICATION EXAMPLE: BIOLOGICAL HIGH-THROUGHPUT DATA. The corresponding vigniette of the old analysis can be found [here](http://bioconductor.org/packages/release/data/experiment/vignettes/IHWpaper/inst/doc/hqtl_IHW_BY.html). In this analysis, the p-values were plotted against the genomic distance and the user had to manually figure out stratification breaks for IHW. Here we want to use the data-driven, automated stratification_methods `"quantile"` and `"forest"`. A draw-back of this is, that we would need to load all 1.6 â‹… 10^10 tests to run the same analysis. This is too massive, so we restrict ourselves to a sub-sample of the hypotheses for the time-being. We will eventually run it with on more computational resources. This is obviously a huge drawback. However, the new stratification breaks are "smarter" and we can incorporate more covariates. So overall, this we will make more discoveries.


Recall that each hypothesis in an hQTL analysis corresponds to a peak of histone modification (which we call gene below) and a SNP. Hence let us the load files about each of the SNPs 

```{r loading_snp_peak}
snp_chr1 <- readRDS(file.path(root_dir, "data/hqtl_chrom1_chrom2/snppos_chr1.Rds")) %>% dplyr::select(snp, pos)
snp_chr2 <- readRDS(file.path(root_dir, "data/hqtl_chrom1_chrom2/snppos_chr2.Rds")) %>% dplyr::select(snp, pos)
head(snp_chr1)
```

and peaks:
```{r}
all_peaks <- readRDS(file.path(root_dir, "data/hqtl_chrom1_chrom2/peak_locations.Rds"))
peaks_chr1 <- dplyr::filter(all_peaks, chr == "chr1") %>% dplyr::select(id, start, end)
peaks_chr2 <- dplyr::filter(all_peaks, chr == "chr2") %>% dplyr::select(id, start, end)
rm(all_peaks)
head(peaks_chr1)
```

We have pre-calculated the p-values for these hypotheses beforehand, we are going to load now.

```{r loading_pvalues}
chr1_df <- readRDS(file.path(root_dir, "data/hqtl_chrom1_chrom2/chr1_subset.Rds")) %>% dplyr::select(SNP, gene, pvalue)
chr2_df <- readRDS(file.path(root_dir, "data/hqtl_chrom1_chrom2/chr2_subset.Rds")) %>% dplyr::select(SNP, gene, pvalue)
```
Note that only p-values <= 1e-4 are stored in these. For details, how the pvalues are calculated, see [code](https://github.com/nignatiadis/IHWpaper/blob/master/inst/real_data_examples/hqtl_example_analysis.R). 
```{r}
pval_threshold <- 10^(-4)
```

# Sub-sample
In the hQTL analysis, we test all possible combinations between peaks and SNPs within a chromosome, e.g.
```{r}
m_full <- 15725016812
paste(nrow(snp_chr1), "*", nrow(peaks_chr1), "+", nrow(snp_chr2), "*", nrow(peaks_chr2), "=", m_full)
```

The full analysis has approximatly 16 billion hypotheses. This is too massive. Simply evaluating `645452 * 12193 + 699343 * 11232` leads to an Integer overflow in R, not to speak of a data.frame with that many rows. To get some progress, we need to sub-sample it. 
```{r, eval = F, include=F}
snp_chr1 <- snp_chr1 %>% slice(1:1000)
snp_chr2 <- snp_chr2 %>% slice(1:1000)

peaks_chr1 <- peaks_chr1 %>% slice(1:10000)
peaks_chr2 <- peaks_chr2 %>% slice(1:10000)
```

Because `chr1_df` only contains p-values <= 1e-4, the below sub-sampling is biased. We will revisit this issue later.
```{r, eval = T, include=T}
chr1_df <- chr1_df %>% slice_sample(n = 1000)
chr2_df <- chr2_df %>% slice_sample(n = 1000)

peaks_chr1 <- peaks_chr1 %>% filter(id %in% chr1_df$gene)
peaks_chr2 <- peaks_chr2 %>% filter(id %in% chr2_df$gene)

snp_chr1 <- snp_chr1 %>% filter(snp %in% chr1_df$SNP)
snp_chr2 <- snp_chr2 %>% filter(snp %in% chr2_df$SNP)
```

```{r, eval = F, include = F}
range(snp_chr1$pos)
range(snp_chr2$pos)

c(min(peaks_chr1$start), min(peaks_chr1$end))
c(min(peaks_chr2$start), min(peaks_chr2$end))
```

# Covariates
## genomic distance
We now take all cross-combinations between the remaining SNPs and peaks with-in chromosmes. We calculate the genomic distance between peaks and SNPs along the way. As [in the original analysis](http://bioconductor.org/packages/release/data/experiment/vignettes/IHWpaper/inst/doc/hqtl_IHW_BY.html), they are going to serve as main covariates for the multiple testing procedure.
```{r}
full_comb_chr1 <- full_join(snp_chr1, peaks_chr1, by = character()) %>%
  transmute(snp, id,
    dist = pmin(abs(pos - start), abs(pos - end))
  )

full_comb_chr2 <- full_join(snp_chr2, peaks_chr2, by = character()) %>%
  transmute(snp, id,
    dist = pmin(abs(pos - start), abs(pos - end))
  )
nrow(full_comb_chr1) + nrow(full_comb_chr2)
```

## Minor Allele Frequency
In [here](http://bioconductor.org/packages/release/data/experiment/vignettes/IHWpaper/inst/doc/hqtl_IHW_BY.html) only the genomic distance was used as covariate. Here, we also want to use minor allele frequency (MAF). We load the pre-downloaded MAF file. The data was pre-downloaded with `Biomart`.
```{r, eval = T}
chr1_maf <- readRDS(file = file.path(root_dir, "data/downloaded_covariates/chr1_maf.Rds"))
chr2_maf <- readRDS(file = file.path(root_dir, "data/downloaded_covariates/chr2_maf.Rds"))
```

Many MAFs are not available on `Biomart`. For IHW, missing covariates are not allowed. So we conservatively replace all `NA`s with 0. 
```{r}
chr1_maf_missing <- sum(is.na(chr1_maf$minor_allele_freq))/nrow(chr1_maf)
chr2_maf_missing <- sum(is.na(chr2_maf$minor_allele_freq))/nrow(chr2_maf)

paste0(round(chr1_maf_missing, 5), "%")
paste0(round(chr2_maf_missing, 5), "%")

chr1_maf <- chr1_maf %>% mutate(minor_allele_freq = replace_na(minor_allele_freq, 0))
chr2_maf <- chr2_maf %>% mutate(minor_allele_freq = replace_na(minor_allele_freq, 0))
```

We annotate the hypotheses with the MAF data.
```{r}
full_comb_chr1 <- full_comb_chr1 %>% left_join(chr1_maf, by = c("snp" = "refsnp_id")) 
full_comb_chr2 <- full_comb_chr2 %>% left_join(chr2_maf, by = c("snp" = "refsnp_id"))
```

```{r cheating, include=FALSE}
#check later
full_comb_chr1 <- full_comb_chr1 %>% mutate(minor_allele_freq = replace_na(minor_allele_freq, 0))
full_comb_chr2 <- full_comb_chr2 %>% mutate(minor_allele_freq = replace_na(minor_allele_freq, 0))
```

We add the pre-calculated p-values to the hypotheses. For the internal parameter `m_groups` of IHW (used to optimize the weights for example) all p-values need to be specified. So we set all missing p-values (e.g. p-values >= 1e-4) to 1. This crude rounding will not affect Benjamini-Hochberg at all and ideally IHW not too much. Note that AdaPT, a competing multiple testing method based on covariates, depends on the precise large p-values to estimate the FDR and hence could not be applied here.
```{r}
chr1_df <- full_comb_chr1 %>%
  left_join(chr1_df, by = (c("snp" = "SNP", "id" = "gene"))) %>%
  transmute(dist, minor_allele_freq,
    pvalue = replace_na(pvalue, 1)
  )

chr2_df <- full_comb_chr2 %>%
  left_join(chr2_df, by = (c("snp" = "SNP", "id" = "gene"))) %>%
  transmute(dist, minor_allele_freq,
    pvalue = replace_na(pvalue, 1)
  )
```

## Checks
We double check, that number of total hypotheses works out.

```{r}
m <- nrow(peaks_chr1) * nrow(snp_chr1) + nrow(snp_chr2) * nrow(peaks_chr2)
m == nrow(chr1_df) + nrow(chr2_df)
m == nrow(full_comb_chr1) + nrow(full_comb_chr2)
m
any(is.na(chr1_df))
any(is.na(chr2_df))
```
So compared to the full analysis, we are only considering a fraction:
```{r}
paste0(round(100 * m / m_full, 5), "%")
```

Again, we are aware that the sub-sampling was biased. Let us put the data for the two chromosomes together:

```{r}
chr1_chr2_df <- rbind(chr1_df, chr2_df)
# folds_vec <- as.factor(c(rep(1, nrow(chr1_df)), rep(2, nrow(chr2_df))))
folds_vec <- c(rep(factor(1), nrow(chr1_df)), rep(factor(2), nrow(chr2_df)))
rm(chr1_df, chr2_df)
rm(snp_chr1, peaks_chr1, snp_chr2, peaks_chr2, full_comb_chr1, full_comb_chr2)
rm(chr1_maf, chr2_maf)
```

# Apply IHW-BY and BY
## Run Multiple testing procedures
We want to apply the Benjamini-Yekutieli at alpha=0.1, thus we will apply Benjamini-Hochberg at the corrected level:

```{r}
alpha <- .01 / (log(m) + 1)
```

```{r, include=FALSE}
#we do not actually want to calculate all the time
run_write_ihw <- TRUE
```

Now let us run IHW with the `"forest"` stratification method using different subsets of the available covariates:

```{r ihw_quantile_dist, include=TRUE, eval=run_write_ihw}
ihw_quantile_dist <- ihw(pvalue ~ dist, data = chr1_chr2_df, alpha, folds = folds_vec)
```

```{r, include=FALSE, eval=run_write_ihw}
saveRDS(ihw_quantile_dist, file = file.path(root_dir, "precomputed_results/ihw_quantile_dist.Rds"))
```

```{r, include=FALSE, eval=TRUE}
ihw_quantile_dist <- readRDS(file = file.path(root_dir, "precomputed_results/ihw_quantile_dist.Rds"))
```

```{r, eval=FALSE, include=F}
# Run some diagnostics on the `ihw_quantile_dist` object relevant when comparing the stratification methods.
m_groups(ihw_quantile_dist)
stratification_breaks(ihw_quantile_dist)
```

```{r ihw_quantile_maf, eval=run_write_ihw}
ihw_quantile_maf <- ihw(pvalue ~ minor_allele_freq, data = chr1_chr2_df, alpha, folds = folds_vec)
```

```{r, include=FALSE, eval=run_write_ihw}
saveRDS(ihw_quantile_maf, file = file.path(root_dir, "precomputed_results/ihw_quantile_maf.Rds"))
```

```{r, include=FALSE, eval=TRUE}
 ihw_quantile_maf <- readRDS(file = file.path(root_dir, "precomputed_results/ihw_quantile_maf.Rds"))
```

```{r ihw_quantile_dist_maf, eval=run_write_ihw}
ihw_quantile_dist_maf <- ihw(pvalue ~ dist + minor_allele_freq, data = chr1_chr2_df, alpha, folds = folds_vec)
```

```{r, include=FALSE, eval=run_write_ihw}
saveRDS(ihw_quantile_dist_maf, file = file.path(root_dir, "precomputed_results/ihw_quantile_dist_maf.Rds"))
```

```{r, include=FALSE, eval=TRUE}
ihw_quantile_dist_maf <- readRDS(file = file.path(root_dir, "precomputed_results/ihw_quantile_dist_maf.Rds"))
```

```{r, eval = F, include=F}
# Run some diagnostics on the `ihw_quantile_dist_maf` object
weights(ihw_quantile_dist_maf)
```

Run IHW with the `"forest"` stratification method using different subsets of the available covariates:
```{r ihw_forest_dist, eval = run_write_ihw, warning= FALSE}
ihw_forest_dist <- ihw(pvalue ~ dist,
  data = chr1_chr2_df,
  alpha, folds = folds_vec, stratification_method = "forest",
  n_censor_thres = 2, ntrees = 1, nodedepth = 4, nodesize = 5000
)
```

```{r, include=FALSE, eval=run_write_ihw}
saveRDS(ihw_forest_dist, file = file.path(root_dir, "precomputed_results/ihw_forest_dist.Rds"))
```

```{r, include=FALSE, eval=TRUE}
ihw_forest_dist <- readRDS(file = file.path(root_dir, "precomputed_results/ihw_forest_dist.Rds"))
```

```{r, eval = F, include=F}
# run some diagnostics on the `ihw_forest` object
IHW::m_groups(ihw_forest_dist)
stratification_breaks(ihw_forest_dist, fold = 1, tree = 1)
```

```{r ihw_forest_maf, eval = run_write_ihw, warning= FALSE}
ihw_forest_maf <- ihw(pvalue ~ minor_allele_freq,
  data = chr1_chr2_df,
  alpha, folds = folds_vec, stratification_method = "forest",
  n_censor_thres = 2, ntrees = 1, nodedepth = 4, nodesize = 1000
)
```

```{r, include=FALSE, eval=run_write_ihw}
saveRDS(ihw_forest_maf, file = file.path(root_dir, "precomputed_results/ihw_forest_maf.Rds"))
```

```{r, include=FALSE, eval=TRUE}
ihw_forest_maf <- readRDS(file = file.path(root_dir, "precomputed_results/ihw_forest_maf.Rds"))
```

```{r ihw_forest_dist_maf, eval = run_write_ihw, warning= FALSE}
ihw_forest_dist_maf <- ihw(pvalue ~ dist + minor_allele_freq,
  data = chr1_chr2_df,
  alpha, folds = folds_vec, stratification_method = "forest",
  n_censor_thres = 2, ntrees = 1, nodedepth = 4, nodesize = 5000
)
```

```{r, include=FALSE, eval=run_write_ihw}
saveRDS(ihw_forest_dist_maf, file = file.path(root_dir, "precomputed_results/ihw_forest_dist_maf.Rds"))
```

```{r, include=FALSE, eval=TRUE}
ihw_forest_dist_maf <- readRDS(file = file.path(root_dir, "precomputed_results/ihw_forest_dist_maf.Rds"))
```

## Evaluate number of rejections
Rejections of BY:

```{r}
sum(p.adjust(na.exclude(chr1_chr2_df$pvalue), n = m, method = "BH") <= alpha)
```

Rejections of IHW-BY using different stratification methods:

```{r}
ihw_rejections <- tribble(
  ~formula, ~quantiles, ~forest,
  "pvalue ~ minor_allele_freq", rejections(ihw_quantile_dist), rejections(ihw_forest_maf),
  "pvalue ~ dist", rejections(ihw_quantile_maf), rejections(ihw_forest_dist),
  "pvalue ~ dist + minor_allele_freq", rejections(ihw_quantile_dist_maf), rejections(ihw_forest_dist_maf),
)
as.data.frame(ihw_rejections)
```

So we see that the number of discoveries has increased in most cases compared to BH. MAF turned out to be not informative, but decreased number of rejections for all stratification methods. However, while IHW-quantile was completely thrown of by an this extra dimension and reduced to classical BY with uniform weights, IHW-Forest was able to handle it.

# Session Info Details
```{r, echo=FALSE, eval=TRUE}
sessionInfo()
```
